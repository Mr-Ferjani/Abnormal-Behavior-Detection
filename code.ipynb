{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"code.ipynb","provenance":[],"collapsed_sections":["DWssHTpFORwx","fWshMQIpOUs8","xND3qfEfSqos","2mQB614GsLLx","91SO_2nzOMtJ"],"authorship_tag":"ABX9TyOlcn/Dhjdom86yFYZ54ymu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DWssHTpFORwx"},"source":["# Imports"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"ztz16GQAIjIa","executionInfo":{"status":"ok","timestamp":1624646972301,"user_tz":-60,"elapsed":85,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"f8bfd9c8-ebd0-4882-e9e4-984c3148a601"},"source":["from IPython.core.display import display, HTML\n","display(HTML(\"<style>.container{max-width:91%!important;width:auto!important;}</style>\"))\n","\n","%reload_ext autoreload\n","%autoreload 2\n","%matplotlib inline"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<style>.container{max-width:91%!important;width:auto!important;}</style>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"8DAhMeotH-mS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624646996911,"user_tz":-60,"elapsed":24691,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"05b643f6-fd0d-415a-fe62-a2f7972d0dae"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir('/content/drive/My Drive/Thesis')\n","folder = os.path.join('/content/drive/My Drive/Thesis')\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","2-STREAM  Auto-encoder\tcode  Fast_load  TensorFlow\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UenB5dHTIzIG"},"source":["import numpy as np\n","import torch as tc  # not an accepted convention\n","import os \n","import math\n","import cv2\n","from tqdm import tqdm,trange\n","import json\n","from pathlib import Path\n","from PIL import Image\n","from torchvision.utils import make_grid\n","from torchvision import datasets, transforms\n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt\n","import mimetypes\n","import torch.nn as nn\n","import torch\n","import random\n","import pandas as pd\n","import pickle\n","mimetypes.init()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hhlE797eOgbz"},"source":["def show_imgs(ims, rows=1, figsize=(16,8), title=[None]):\n","    title = title*len(ims) if len(title) == 1 else title\n","    _,ax = plt.subplots(rows, len(ims)//rows, figsize=figsize)\n","    [show_img(im,ax_,title=tit) for im,ax_,tit in zip(ims,ax.flatten(),title)]\n","    return ax"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uAVuQi0AOhZ9"},"source":["def show_img(im, ax=None, figsize=(8,8), title=None):\n","    if not ax: _,ax = plt.subplots(1,1,figsize=figsize)\n","    if len(im.shape)==2: im = np.tile(im[:,:,None], 3) \n","    ax.imshow(im);\n","    ax.xaxis.set_visible(False)\n","    ax.yaxis.set_visible(False)\n","    if title: ax.set_title(title)\n","    return ax"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"voxANL0PwjIh"},"source":["def denorm(x, μ, σ):\n","    return (x * σ )+ μ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xRq2pj_XktN8"},"source":["def seed_everything(seed=2021):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","seed_everything()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fWshMQIpOUs8"},"source":["# Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"wk3paTyOOs10"},"source":["The function below takes a video file and extracts and saves all frames in the designated directory."]},{"cell_type":"code","metadata":{"id":"QQDVeCG8OWVq"},"source":["def Imgs_Extractor(video_file, path='./', ext='.mp4', frames_dir='train_dir',frames_ext='.jpg'):\n","    os.makedirs(frames_dir, exist_ok=True) #creating directory for training or testing data \n","    if extension not in video_file:\n","        video_file += extension #checking the of video file extension \n","    cap = cv2.VideoCapture(path+video_file)   \n","    frame_Rate = cap.get(5) #frame rate\n","    os.makedirs(frames_dir+'/'+video_file, exist_ok=True) #creating directory per video\n","    counter = 0\n","    while(cap.isOpened()):\n","        frame_ID = cap.get(1) #current frame number\n","        ret, frame = cap.read() #reading frame and returning flag\n","        if (ret != True):\n","            break\n","        #Storing frames in frames_dir\n","        filename = frames_dir + '/' + video_file + '/' + video_file + '_frame{0}'.format(count)+frames_ext #frame label\n","        cv2.imwrite(filename,frame) #saving frame\n","        counter += 1\n","    cap.release()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mPIJzT-7O6TL"},"source":["Function that extracts paths corresponding to images only."]},{"cell_type":"code","metadata":{"id":"itAhJWv-O8v0"},"source":["def get_extensions_for_type(general_type):\n","    for ext in mimetypes.types_map:\n","        if mimetypes.types_map[ext].split('/')[0] == general_type:\n","            yield ext"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bPruy6ZZO_cj"},"source":["#Contains all possible extensions of images\n","IMAGE = tuple(get_extensions_for_type('image'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VMzshOrLPCyG"},"source":["Function that gets all file names per specified directory path."]},{"cell_type":"code","metadata":{"id":"R5fwu81xPBX3"},"source":["def get_files(path,Img_ext=IMAGE,flag_all=False):\n","     path = Path(path)\n","     if flag_all == False: \n","        return [x for x in path.iterdir() if '.'+x.parts[-1].split('.')[-1] in Img_ext] #getting only paths that correspond to images\n","     else:\n","        return [x for x in path.iterdir()] #getting all paths\n","     "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2p7JFTwKPOc9"},"source":["Preprocessing stage"]},{"cell_type":"code","metadata":{"id":"sxz5LlYbPQus"},"source":["def all_paths(path,Img_ext):\n","    all_paths = [] #will contain all frames path of selected dataset\n","    all_vids = get_files(path,Img_ext,True) #gets all video paths of the dataset\n","    for vid in all_vids:\n","        all_frames = get_files(vid,Img_ext,False) #gets all frames paths of the dataset\n","        for frame in all_frames:\n","           all_paths.append(frame)\n","    return all_paths"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mH15xzeAPSfr"},"source":["def Preprocessing(path,Img_ext,shape=(224,224)):\n","    paths=all_paths(path,Img_ext)\n","    #Image stacking\n","    Img_list = []\n","    for filename in tqdm(paths):\n","        img = Image.open(filename)\n","        img = img.resize(shape)\n","        img = np.array(img)\n","        Img_list.append(img)\n","    return Img_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ot6YXCjKiOR4","executionInfo":{"status":"ok","timestamp":1624647469827,"user_tz":-60,"elapsed":449,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"ef06d0b9-3bd7-4533-e2cf-dd06d01ee6d4"},"source":["p = os.getcwd()\n","p"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/Thesis'"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"B9MSLIpWgsa1","executionInfo":{"status":"ok","timestamp":1624647527060,"user_tz":-60,"elapsed":355,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"427f1f1a-2eb9-499f-aa5a-02c40a0d2c3c"},"source":["p = os.getcwd()\n","path = os.path.join(p,'code/Datasets/UCSDped1/UCSDped1/Test')\n","path"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/Thesis/code/Datasets/UCSDped1/UCSDped1/Test'"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"RWQMy4ersqiV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624649555375,"user_tz":-60,"elapsed":636631,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"ccd78273-ebcf-4578-e90d-a86107278e4c"},"source":["Img_list = Preprocessing(path,IMAGE,(227,227))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 7200/7200 [31:29<00:00,  3.81it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"6k4JQeouiubO"},"source":["#p = os.getcwd()\n","#path = os.path.join(p,'testing_set_227x227.pkl')\n","#with open(path,'wb') as f: pickle.dump(Img_list, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DTZsyoKX-kr3"},"source":["#Read training Data\n","p = os.getcwd()\n","path = os.path.join(p,'Fast_load/UCSDped1_train')\n","Img_list_training = pd.read_pickle (path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WSqDobqAoOIx"},"source":["#Read Testing Data\n","#p = os.getcwd()\n","#path = os.path.join(p,'Fast_load/UCSDped1_test.pkl')\n","#Img_list_testing = pd.read_pickle(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Z_Xvo_EUVDq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624224950691,"user_tz":-60,"elapsed":1589,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"245a6242-5d37-442f-dd50-23d1057810dc"},"source":["#Calculating mean μ and standard deviation σ.\n","numpy_Img_list_training = np.array(Img_list_training,dtype=np.float32)/255\n","μ = numpy_Img_list_training.mean(); σ = numpy_Img_list_training.std()\n","print(f'Data mean before normalization = {μ}, Data Std before normalization = {σ}')\n","#normlized_dataset = (numpy_Img_list-numpy_Img_list.mean())/numpy_Img_list.std()\n","#print(f'Data mean after normalization = {normlized_dataset.mean()}, Data Std after normalization = {normlized_dataset.std()}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Data mean before normalization = 0.3750261962413788, Data Std before normalization = 0.2031460851430893\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YNacq0qE5FGn"},"source":["#numpy_Img_list_testing = np.array(Img_list_testing,dtype=np.float32)/255"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9K2bAqJown43"},"source":["#Mean and Std of training dataset:\n","μ,σ = 0.375,0.203"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ifbhRrhR9UBl","executionInfo":{"status":"ok","timestamp":1623775788769,"user_tz":-60,"elapsed":588,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"0e2acaa9-bb46-486c-d48f-145eeaa8354d"},"source":["print(f'number of frames for training dataset = {len(numpy_Img_list)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["number of frames for training dataset = 6800\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z1ut_gA1PWbz"},"source":["def Normalization(Img_list,μ,σ):\n","    #Img_list=np.array(Img_list,dtype=np.float32)\n","    batch,height,width = Img_list.shape\n","    #Reshape to (224,224,batch_size)\n","    Img_list.resize(height,width,batch)\n","    #Normalize\n","    #Img_list=(Img_list-μ)/(σ)\n","    #Clip negative Values\n","    Img_list=np.clip(Img_list,0,1)\n","    #Final Reshaping\n","    Img_list.resize(batch,height,width)\n","    data = np.zeros((int(batch/10),10,height,width),dtype=np.float32)\n","    i, index = 0, 0\n","    for t in trange(batch):\n","        data[index,i,:,:] = (Img_list[t,...]-μ)/(σ)#/255\n","        i+=1\n","        if i%10 == 0 and i!=0:\n","            i=0;index+=1\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_eVsxqSJhSS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624224956650,"user_tz":-60,"elapsed":1624,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"b696c2aa-74b6-466d-e7fb-c51f6df8e0da"},"source":["tr_ds= Normalization(numpy_Img_list_training,μ,σ)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 6800/6800 [00:00<00:00, 14591.79it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IEH1YQXM5AJ2","executionInfo":{"status":"ok","timestamp":1623965929864,"user_tz":-60,"elapsed":2147,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"d66962e6-9afa-4b3d-faed-eaffd48d07bc"},"source":["#val_ds= Normalization(numpy_Img_list_testing,μ,σ)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 7200/7200 [00:00<00:00, 13091.68it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gudAV2moHsBe","executionInfo":{"status":"ok","timestamp":1624224961028,"user_tz":-60,"elapsed":253,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"be8718e0-ad6d-4452-f921-73d5dc42dd60"},"source":["tr_ds.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(680, 10, 224, 224)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-SSWOyeG-tM0","executionInfo":{"status":"ok","timestamp":1624224963469,"user_tz":-60,"elapsed":269,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"5358398e-b0e3-4961-9a27-f4cec25d5db1"},"source":["#shape of dataset\n","tr_ds = np.expand_dims(tr_ds, axis=1)\n","tr_ds.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(680, 1, 10, 224, 224)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mJtwoPai6wO7","executionInfo":{"status":"ok","timestamp":1624224965245,"user_tz":-60,"elapsed":255,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"77443afe-7531-49e3-f251-51bf2b6d0741"},"source":["tr_ds.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(680, 1, 10, 224, 224)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uuc6X35n6pSS","executionInfo":{"status":"ok","timestamp":1623966039669,"user_tz":-60,"elapsed":257,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"f8a01b04-e5d5-43da-9ac8-86fe709397be"},"source":["#val_ds = np.expand_dims(val_ds, axis=1)\n","#val_ds.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(720, 1, 10, 224, 224)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7qIVEMv0Fv8J","executionInfo":{"status":"ok","timestamp":1623968913541,"user_tz":-60,"elapsed":286,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"202cd903-a1af-4e64-8915-374fec9bba50"},"source":["#val_ds.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([720, 1, 10, 224, 224])"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"07KYPXpIS3Pi","executionInfo":{"status":"ok","timestamp":1623761441876,"user_tz":-60,"elapsed":223,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"7ada0758-dcbf-4370-c2fb-2fc76ae993dc"},"source":["print(f'mean = {μ} ,Std = {σ}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mean = 0.37718164920806885 ,Std = 0.20455783605575562\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"w7Y3S84AeXYb"},"source":["Training and Testing datasets"]},{"cell_type":"code","metadata":{"id":"bf5PWL9neaMc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624224970514,"user_tz":-60,"elapsed":268,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"24ba4e64-b07b-4f73-e63b-785dedde7843"},"source":["tr_ds = tc.from_numpy(tr_ds)#;val_ds = tc.from_numpy(val_ds) #Transforming from numpy to tensor\n","tr_ds = tr_ds.type(tc.FloatTensor)#;val_ds = val_ds.type(tc.FloatTensor) #Casting tensors to Float32\n","tr_ds.dtype#,val_ds.dtype"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.float32"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"wqUtRAiofTBg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623527844448,"user_tz":-60,"elapsed":338,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"9ca6f41b-2468-4793-cae2-108078e935aa"},"source":["#Size of tensors: a.element_size() * a.nelement().\n","tr_ds.element_size()*8*224*224*10"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16056320"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"xND3qfEfSqos"},"source":["# Auto-Encoder Model"]},{"cell_type":"code","metadata":{"id":"lF4uTCzZIaSL"},"source":["class ConvLSTMCell(nn.Module):\n","\n","    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n","        \"\"\"\n","        Initialize ConvLSTM cell.\n","\n","        Parameters\n","        ----------\n","        input_dim: int\n","            Number of channels of input tensor.\n","        hidden_dim: int\n","            Number of channels of hidden state.\n","        kernel_size: (int, int)\n","            Size of the convolutional kernel.\n","        bias: bool\n","            Whether or not to add the bias.\n","        \"\"\"\n","\n","        super(ConvLSTMCell, self).__init__()\n","\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","\n","        self.kernel_size = kernel_size\n","        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n","        self.bias = bias\n","\n","        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n","                              out_channels=4 * self.hidden_dim,\n","                              kernel_size=self.kernel_size,\n","                              padding=self.padding,\n","                              bias=self.bias)\n","\n","    def forward(self, input_tensor, cur_state):\n","        h_cur, c_cur = cur_state\n","\n","        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n","\n","        combined_conv = self.conv(combined)\n","        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n","        i = torch.sigmoid(cc_i)\n","        f = torch.sigmoid(cc_f)\n","        o = torch.sigmoid(cc_o)\n","        g = torch.tanh(cc_g)\n","\n","        c_next = f * c_cur + i * g\n","        h_next = o * torch.tanh(c_next)\n","\n","        return h_next, c_next\n","\n","    def init_hidden(self, batch_size, image_size):\n","        height, width = image_size\n","        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n","                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6h0_cIuLjbZ"},"source":["class ConvLSTM(nn.Module):\n","\n","    \"\"\"\n","\n","    Parameters:\n","        input_dim: Number of channels in input\n","        hidden_dim: Number of hidden channels\n","        kernel_size: Size of kernel in convolutions\n","        num_layers: Number of LSTM layers stacked on each other\n","        batch_first: Whether or not dimension 0 is the batch or not\n","        bias: Bias or no bias in Convolution\n","        return_all_layers: Return the list of computations for all layers\n","        Note: Will do same padding.\n","\n","    Input:\n","        A tensor of size B, T, C, H, W or T, B, C, H, W\n","    Output:\n","        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n","            0 - layer_output_list is the list of lists of length T of each output\n","            1 - last_state_list is the list of last states\n","                    each element of the list is a tuple (h, c) for hidden state and memory\n","    Example:\n","        >> x = torch.rand((32, 10, 64, 128, 128))\n","        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n","        >> _, last_states = convlstm(x)\n","        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n","    \"\"\"\n","\n","    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n","                 batch_first=False, bias=True, return_all_layers=False):\n","        super(ConvLSTM, self).__init__()\n","\n","        self._check_kernel_size_consistency(kernel_size)\n","\n","        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n","        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n","        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n","        if not len(kernel_size) == len(hidden_dim) == num_layers:\n","            raise ValueError('Inconsistent list length.')\n","\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.kernel_size = kernel_size\n","        self.num_layers = num_layers\n","        self.batch_first = batch_first\n","        self.bias = bias\n","        self.return_all_layers = return_all_layers\n","\n","        cell_list = []\n","        for i in range(0, self.num_layers):\n","            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n","\n","            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n","                                          hidden_dim=self.hidden_dim[i],\n","                                          kernel_size=self.kernel_size[i],\n","                                          bias=self.bias))\n","\n","        self.cell_list = nn.ModuleList(cell_list)\n","\n","    def forward(self, input_tensor, hidden_state=None):\n","        \"\"\"\n","\n","        Parameters\n","        ----------\n","        input_tensor: todo\n","            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n","        hidden_state: todo\n","            None. todo implement stateful\n","\n","        Returns\n","        -------\n","        last_state_list, layer_output\n","        \"\"\"\n","        if not self.batch_first:\n","            # (t, b, c, h, w) -> (b, t, c, h, w)\n","            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n","\n","        b, _, _, h, w = input_tensor.size()\n","\n","        # Implement stateful ConvLSTM\n","        if hidden_state is not None:\n","            raise NotImplementedError()\n","        else:\n","            # Since the init is done in forward. Can send image size here\n","            hidden_state = self._init_hidden(batch_size=b,\n","                                             image_size=(h, w))\n","\n","        layer_output_list = []\n","        last_state_list = []\n","\n","        seq_len = input_tensor.size(1)\n","        cur_layer_input = input_tensor\n","\n","        for layer_idx in range(self.num_layers):\n","\n","            h, c = hidden_state[layer_idx]\n","            output_inner = []\n","            for t in range(seq_len):\n","                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n","                                                 cur_state=[h, c])\n","                output_inner.append(h)\n","\n","            layer_output = torch.stack(output_inner, dim=1)\n","            cur_layer_input = layer_output\n","\n","            layer_output_list.append(layer_output)\n","            last_state_list.append([h, c])\n","\n","        if not self.return_all_layers:\n","            layer_output_list = layer_output_list[-1:]\n","            last_state_list = last_state_list[-1:]\n","\n","        return layer_output_list, last_state_list #------------------------------layer_output_list[0]\n","\n","    def _init_hidden(self, batch_size, image_size):\n","        init_states = []\n","        for i in range(self.num_layers):\n","            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n","        return init_states\n","\n","    @staticmethod\n","    def _check_kernel_size_consistency(kernel_size):\n","        if not (isinstance(kernel_size, tuple) or\n","                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n","            raise ValueError('`kernel_size` must be tuple or list of tuples')\n","\n","    @staticmethod\n","    def _extend_for_multilayer(param, num_layers):\n","        if not isinstance(param, list):\n","            param = [param] * num_layers\n","        return param"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ehs5f2wgqkRU"},"source":["class AutoEncoder(nn.Module): \n","    def __init__(self):\n","        super().__init__()\n","        #Spatial Encoder\n","        self.spatial_encoder = nn.Sequential(\n","             nn.Conv3d(in_channels=1,  out_channels=128, kernel_size=(1,11,11), stride=(1,4,4), padding=(0,4,4),dilation=(1,1,1)),\n","             nn.BatchNorm3d(128),\n","             nn.Tanh(),\n","             nn.Conv3d(in_channels=128,  out_channels=256, kernel_size=(1,7,7), stride=(1,2,2), padding=(0,3,3)),\n","             nn.BatchNorm3d(256),\n","             nn.Tanh(),\n","             nn.Conv3d(in_channels=256,  out_channels=64, kernel_size=(1,5,5), stride=(1,2,2), padding=(0,2,2)),\n","             nn.BatchNorm3d(64),\n","             nn.Tanh()\n","            \n","            )\n","        #Convolutional_LSTM\n","        self.convlstm_stacked = ConvLSTM(input_dim=64,\n","                 hidden_dim=[64, 32, 64],\n","                 kernel_size=(3, 3),\n","                 num_layers=3,\n","                 batch_first=True,\n","                 bias=True,\n","                 return_all_layers=False)\n","        #Spatial Decoder\n","        self.spatial_decoder = nn.Sequential(\n","             nn.ConvTranspose3d(in_channels=64,  out_channels=256, kernel_size=(1,5,5), stride=(1,2,2), padding=(0,2,2),output_padding=(0,1,1)),\n","             nn.BatchNorm3d(256),\n","             nn.Tanh(),\n","             nn.ConvTranspose3d(in_channels=256,  out_channels=128, kernel_size=(1,7,7), stride=(1,2,2), padding=(0,3,3),output_padding=(0,1,1)),\n","             nn.BatchNorm3d(128),\n","             nn.Tanh(),\n","             nn.ConvTranspose3d(in_channels=128,  out_channels=1, kernel_size=(1,11,11), stride=(1,4,4), padding=(0,4,4),output_padding=(0,1,1)),\n","             #nn.BatchNorm3d(1),\n","             nn.Tanh(), \n","            \n","        )\n","            \n","    def forward(self, x): \n","        # here we define forward-pass logic\n","        #input of shape = (b, c, d , h, w)\n","        x = self.spatial_encoder(x)\n","        #(b, c, d , h, w) ---> (b, d, c, h, w)\n","        x = x.permute(0, 2, 1, 3, 4)\n","        x, last_states = self.convlstm_stacked(x)\n","        #(b, d, c, h, w) ---> (b, c, d , h, w)\n","        X = x[0]\n","        X = X.permute(0, 2, 1, 3, 4)\n","        X = self.spatial_decoder(X)\n","        return X"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xtyydBYwxqtg"},"source":["Visualize"]},{"cell_type":"markdown","metadata":{"id":"2PpWYw2mKe1K"},"source":["# Training/Testing"]},{"cell_type":"markdown","metadata":{"id":"NWLGeMSQM6XJ"},"source":["one-epoch function that either trains or validates depending on input."]},{"cell_type":"code","metadata":{"id":"uf0c75txKh8j"},"source":["# cell no 5\n","def one_epoch(net, loss, dl, opt=None, metric=None):\n","    \n","    if opt:\n","        net.train()  # only affects some layers\n","    else:\n","        net.eval()\n","        rq_stored = []\n","        for p in net.parameters():\n","            rq_stored.append(p.requires_grad)\n","            p.requires_grad = False\n","    \n","    L = []\n","    dl_it = iter(dl)\n","    for xb in tqdm(dl_it, leave=False):\n","        xb = xb.to(device)\n","        x_ = net(xb)\n","        l = loss(x_, xb)\n","        if opt:\n","            opt.zero_grad()\n","            l.backward()\n","            opt.step()\n","        L.append(l.detach().cpu().numpy())\n","        \n","        \n","    if not opt:\n","        for p,rq in zip(net.parameters(), rq_stored): p.requires_grad = rq\n","            \n","    return L    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BJHIqLhezxqL"},"source":["Model"]},{"cell_type":"code","metadata":{"id":"hsL2C08Gzw7Y"},"source":["device = tc.device('cuda' if tc.cuda.is_available() else 'cpu')\n","#model = AutoEncoder().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KH8FI0F1QtsE"},"source":["Hyper-parameters"]},{"cell_type":"code","metadata":{"id":"CwsV82FCRKWk"},"source":["lr=1e-3 \n","wd=1e-3 \n","epoch=10 \n","loss=nn.L1Loss() \n","batch_size = 8 "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zqnagzq4ax1b"},"source":["Data Loaders"]},{"cell_type":"code","metadata":{"id":"bnDXCjwxazmS"},"source":["tr_dl  = DataLoader(tr_ds,  batch_size=batch_size, shuffle=True,  num_workers=2)\n","#val_dl  = DataLoader(val_ds,  batch_size=batch_size, shuffle=False,  num_workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DT_LCb0qNjxp"},"source":["Fit function"]},{"cell_type":"code","metadata":{"id":"snbINr18NuH3"},"source":["# cell no 7\n","def fit(net, tr_dl, loss=nn.CrossEntropyLoss(), epochs=3, lr=3e-3, wd=1e-3, plot=True):   \n","    \n","    #opt = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n","    opt = tc.optim.SGD( model.parameters(),lr= lr,momentum=0.9,weight_decay=wd) #SGD OPtimizer\n","    \n","    Ltr_hist, Lval_hist = [], []\n","    for epoch in trange(epochs):\n","        Ltr = one_epoch(net, loss, tr_dl,  opt)\n","        #Lval = one_epoch(net, loss, val_dl, None)\n","        Ltr_hist.append(np.mean(Ltr))\n","        #Lval_hist.append(np.mean(Lval))\n","        #print(f'epoch: {epoch}\\ttraining loss: {np.mean(Ltr):0.4f}\\tvalidation loss: {np.mean(Lval):0.4f}')\n","        print(f'epoch: {epoch+1}\\ttraining loss: {np.mean(Ltr):0.4f}')\n","        \n","    # plot the losses     \n","    if plot:\n","        _,ax = plt.subplots(1,1,figsize=(16,4))\n","        ax.plot(1+np.arange(len(Ltr_hist)),Ltr_hist)\n","        #ax.plot(1+np.arange(len(Lval_hist)),Lval_hist)\n","        ax.grid('on')\n","        ax.set_xlim(left=1, right=len(Ltr_hist))\n","        ax.legend(['training loss']);\n","        #ax.legend(['training loss', 'validation loss']);\n","        \n","    return Ltr_hist#,Lval_hist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NbAKsj0qsKVc"},"source":["Loss_tr = fit(model, tr_dl, loss, epoch, lr, wd, True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXumT295spTO"},"source":["Save Loss values of training"]},{"cell_type":"code","metadata":{"id":"LxwZrWURvP5N"},"source":["p = os.getcwd()\n","path = os.path.join(p,'LOSS-MAE_SGD_80E_lr1_bs8_BN.pkl')\n","with open(path,'wb') as f: pickle.dump(Loss_tr, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nzh3sVKBU05s"},"source":["Saving Model"]},{"cell_type":"code","metadata":{"id":"MkQCZs2lUXoh"},"source":["#Saving\n","checkpoint = {\n","\"epoch\": 80,\n","\"model_state\": model.state_dict(),\n","#\"optim_state\": optim.state_dict()\n","}\n","#print(optim.state_dict())\n","FILE = \"SGD_checkpoint_Epoch80.pth\"\n","torch.save(checkpoint, FILE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oPD8kpulwInr"},"source":["Loading Model"]},{"cell_type":"code","metadata":{"id":"rxJeXP7oxnlk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624225042630,"user_tz":-60,"elapsed":12793,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"7f6b4ac5-df82-478c-abae-3e9a8f205999"},"source":["#Loading\n","p = os.getcwd()\n","FILE = os.path.join(p,'2-STREAM/SGD_checkpoint_Epoch50.pth')\n","model = AutoEncoder()\n","checkpoint = torch.load(FILE)\n","model.load_state_dict(checkpoint['model_state'])\n","model.to(device)\n","epoch = checkpoint['epoch']\n","print(f'checkpoint epoch = {epoch}')\n","#model.eval()\n","# - or -\n","# model.train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["checkpoint epoch = 50\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YPhB7xVUkwM2"},"source":["Loss_tr = fit(model, tr_dl, loss, epoch, lr, wd, True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RnSp7sQRgtps"},"source":["Plot Regularity score."]},{"cell_type":"code","metadata":{"id":"eeN9kMImgz9Z"},"source":["\n","def Regularity_score(net,dl):\n","  \"\"\"\n","  Regularity Score = 1 - (e_t - min@t(e_t))/max@t(e_t)\n","  \"\"\"\n","  e_t = []\n","  dl_it = iter(dl)\n","  net.eval()\n","  for X in tqdm(dl_it,leave=False):\n","     output = net(X)\n","     X = X.numpy().squeeze()\n","     output = output.detach().numpy().squeeze()\n","     bs,depth,height,width = X.shape[0],X.shape[-3],X.shape[-2],X.shape[-1]\n","     for B in trange(bs):\n","       for i in trange(depth):\n","         #TODO\n","         a = X[B,i,:,:].reshape(height*width,1)\n","         b = output[B,i,:,:].reshape(height*width,1)\n","         e_xyt = np.linalg.norm(a-b,axis=0,ord=2)\n","         e_t.append(e_xyt[0])\n","\n","  e_t_min = min(e_t)\n","  e_t_max = max(e_t)\n","  reg_scores = []\n","  for i in trange(len(e_t)):\n","     reg_scores.append(1 - ((e_t[i]-e_t_min)/e_t_max))\n","  \n","  return reg_scores "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2pvoFTky3-Ix"},"source":["#reg_scores=Regularity_score(model,dl)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rhSmE6x6zlMc"},"source":["Plotting the Regularity Score"]},{"cell_type":"code","metadata":{"id":"a3WDLetbzi2S"},"source":["def plot(reg_scores):\n","  _,ax = plt.subplots(1,1,figsize=(16,4))\n","  ax.plot(1+np.arange(len(reg_scores)),reg_scores,color = 'red')\n","  ax.grid('on')\n","  ax.set_xlabel(\"frame number\")\n","  ax.set_ylabel(\"regularity score\")\n","  ax.set_title( \"Regularity Score per frame\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2mQB614GsLLx"},"source":["#Save and Load Checkpoint"]},{"cell_type":"code","metadata":{"id":"5pCcD493tlql"},"source":["#Saving\n","checkpoint = {\n","\"epoch\": 30,\n","\"model_state\": model.state_dict(),\n","#\"optim_state\": optim.state_dict()\n","}\n","#print(optim.state_dict())\n","FILE = \"ch_checkpoint_Epoch30.pth\"\n","torch.save(checkpoint, FILE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IXqmrmopuiTU"},"source":["#Loading\n","p = os.getcwd()\n","FILE = os.path.join(p,'Fast_load/checkpoint_Epoch20.pth')\n","model = AutoEncoder()\n","#optim = torch.optim.SGD(model.parameters(), lr= lr,momentum=0.9,weight_decay=wd)\n","checkpoint = torch.load(FILE)\n","model.load_state_dict(checkpoint['model_state'])\n","model.to(device)\n","#optimizer.load_state_dict(checkpoint['optim_state'])\n","#epoch = checkpoint['epoch']\n","#model.eval()\n","# - or -\n","# model.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JaJ093y8sNqB"},"source":["#p = os.getcwd()\n","#path = os.path.join(p,'Fast_load/loss_sgd_10E_lr1_bs4_BN.pth')\n","#Img_list = pd.read_pickle(path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DHuQpeaDtWkQ"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"91SO_2nzOMtJ"},"source":["# Trials"]},{"cell_type":"code","metadata":{"id":"-N2Eich2L4X7"},"source":["path = '/content/drive/My Drive/Thesis/code/005.tif'\n","img = Image.open(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kuB-wpaqI2VE","executionInfo":{"status":"ok","timestamp":1622661958318,"user_tz":-60,"elapsed":238,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"e270a8a0-d564-46e1-8ef4-3d1a4161e0e1"},"source":["p = os.getcwd()\n","path = os.path.join(p,'code/005.tif')\n","path\n","img = Image.open(path)\n","img = img.resize((227,227))\n","img.size"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(227, 227)"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"77M8qFxdpRVR"},"source":["#-----------------------------TRIAL-------------------------------------------\n","class trial(nn.Module): \n","    def __init__(self):\n","        super().__init__()\n","        self.spatial_encoder = nn.Sequential(\n","             nn.Conv3d(in_channels=1,  out_channels=128, kernel_size=(1,11,11), stride=(1,4,4), padding=(0,2,2)),\n","             nn.BatchNorm3d(128),\n","             nn.Tanh(),\n","            )\n","        self.temporal_encoder = nn.Sequential()\n","        self.spatial_decoder = nn.Sequential(\n","             nn.Conv3d(in_channels=64,  out_channels=256, kernel_size=(1,5,5), stride=(1,2,2), padding=1),\n","             nn.BatchNorm3d(256),\n","             nn.Tanh(),\n","             nn.Conv3d(in_channels=256,  out_channels=128, kernel_size=(1,7,7), stride=(1,2,2), padding=2),\n","             nn.BatchNorm3d(128),\n","             nn.Tanh(),\n","             nn.Conv3d(in_channels=128,  out_channels=1, kernel_size=(1,11,11,), stride=(1,4,4), padding=2),\n","             nn.BatchNorm3d(1),\n","             nn.Tanh(),\n","            \n","        )\n","            \n","    def forward(self, x): \n","        # here we define forward-pass logic\n","        x = self.spatial_encoder(x)\n","        #x, last_states = self.convlstm_1(x)\n","        #x, last_states = self.convlstm_2(x)\n","        #x, last_states = self.convlstm_3(x)\n","        #x = self.spatial_decoder(x)\n","        return x\n","#--------------------------------------------------------------TRIAL------------------------------------------------------------------"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z2Y7feyx2u7_"},"source":["spatial_decoder = nn.Sequential(\n","             nn.ConvTranspose3d(in_channels=64,  out_channels=256, kernel_size=(1,5,5), stride=(1,2,2), padding=(0,2,2),output_padding=(0,1,1)),\n","             nn.BatchNorm3d(256),\n","             nn.Tanh(),\n","             nn.ConvTranspose3d(in_channels=256,  out_channels=128, kernel_size=(1,7,7), stride=(1,2,2), padding=(0,3,3),output_padding=(0,1,1)),\n","             nn.BatchNorm3d(128),\n","             nn.Tanh(),\n","             nn.ConvTranspose3d(in_channels=128,  out_channels=1, kernel_size=(1,11,11), stride=(1,4,4), padding=(0,4,4),output_padding=(0,1,1)),\n","             #nn.BatchNorm3d(1),\n","             nn.Tanh()\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aMPdS6fm2x_n","executionInfo":{"status":"ok","timestamp":1623183276502,"user_tz":-60,"elapsed":696,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"59238632-bd54-4c59-e8f9-0e5409d3bffe"},"source":["#(b, c, d , h, w)\n","#x_decode = torch.rand((1, 64, 10, 14, 14))\n","x_decode = torch.rand((1, 64, 10, 14, 14))\n","outy = spatial_decoder(x_decode)\n","outy.size()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 1, 10, 224, 224])"]},"metadata":{"tags":[]},"execution_count":218}]},{"cell_type":"code","metadata":{"id":"lThE8tsVGdXR"},"source":["spatial_encoder = nn.Sequential(\n","             nn.Conv3d(in_channels=1,  out_channels=128, kernel_size=(1,11,11), stride=(1,4,4), padding=(0,4,4),dilation=(1,1,1)),\n","             nn.BatchNorm3d(128),\n","             nn.Tanh(),\n","             nn.Conv3d(in_channels=128,  out_channels=256, kernel_size=(1,7,7), stride=(1,2,2), padding=(0,2,2),dilation=(1,1,1)),\n","             nn.BatchNorm3d(256),\n","             nn.Tanh(),\n","             nn.Conv3d(in_channels=256,  out_channels=64, kernel_size=(1,5,5), stride=(1,2,2), padding=(0,2,2)),\n","             nn.BatchNorm3d(64),\n","             nn.Tanh()\n","            )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gh8xR50KH7mh"},"source":["#(b, c, d , h, w)\n","x_encode = torch.rand((1, 1, 10, 224, 224))\n","outy = spatial_encoder(x_encode)\n","outy.size()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i6574p1tz2Mj"},"source":["#(b, c, d , h, w) ---> (b, d, c, h, w)\n","x_encode = torch.rand((1, 64, 10, 224, 224))\n","x_encode = x_encode.permute(0, 2, 1, 3, 4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wITCWzP4ziik"},"source":["model = ConvLSTM(input_dim=64,\n","                 hidden_dim=[64, 32, 64],\n","                 kernel_size=(3, 3),\n","                 num_layers=3,\n","                 batch_first=True,\n","                 bias=True,\n","                 return_all_layers=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-GwHrzON0PD9"},"source":["h_next,states = model(x_encode)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SdVK6k5C0kWt","executionInfo":{"status":"ok","timestamp":1623276936635,"user_tz":-60,"elapsed":264,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"23cec4f4-911d-4b8f-dfe5-444667058085"},"source":["len(states[0]),len(h_next)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2, 1)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"p60njWcDNgfr"},"source":["class AutoEncoder(nn.Module): \n","    def __init__(self):\n","        super().__init__()\n","        #Spatial Encoder\n","        self.spatial_encoder = nn.Sequential(\n","             nn.Conv3d(in_channels=1,  out_channels=128, kernel_size=(1,11,11), stride=(1,4,4), padding=(0,2,2)),\n","             nn.BatchNorm3d(128),\n","             nn.Tanh(),\n","             nn.Conv3d(in_channels=128,  out_channels=256, kernel_size=(1,7,7), stride=(1,2,2), padding=(0,1,1)),\n","             nn.BatchNorm3d(256),\n","             nn.Tanh(),\n","             nn.Conv3d(in_channels=256,  out_channels=64, kernel_size=(1,5,5), stride=(1,2,2), padding=(0,1,1)),\n","             nn.BatchNorm3d(64),\n","             nn.Tanh()\n","            )\n","        #LSTM\n","        self.convlstm_1 = ConvLSTM(64, 32, (3,3), 1, True, True, False)\n","        self.convlstm_2 = ConvLSTM(32, 32, (3,3), 1, True, True, False)\n","        self.convlstm_3 = ConvLSTM(32, 64, (3,3), 1, True, True, False)\n","        #Spatial Decoder\n","        self.spatial_decoder = nn.Sequential(\n","             nn.ConvTranspose3d(in_channels=64,  out_channels=256, kernel_size=(1,5,5), stride=(1,2,2), padding=(0,1,1)),\n","             nn.BatchNorm3d(256),\n","             nn.Tanh(),\n","             nn.ConvTranspose3d(in_channels=256,  out_channels=128, kernel_size=(1,7,7), stride=(1,2,2), padding=(0,2,2)),\n","             nn.BatchNorm3d(128),\n","             nn.Tanh(),\n","             nn.ConvTranspose3d(in_channels=128,  out_channels=1, kernel_size=(1,11,11), stride=(1,4,4), padding=(0,2,2)),\n","             #nn.BatchNorm3d(1),\n","             nn.Tanh(),\n","            \n","        )\n","            \n","    def forward(self, x): \n","        # here we define forward-pass logic\n","        x = self.spatial_encoder(x)\n","        #(b, c, d , h, w) ---> (b, d, c, h, w)\n","        x = x.permute(0, 2, 1, 3, 4)\n","        x, last_states = self.convlstm_1(x)\n","        x, last_states = self.convlstm_2(x)\n","        x, last_states = self.convlstm_3(x)\n","        #(b, d, c, h, w) ---> (b, c, d , h, w)\n","        x = x.permute(0, 2, 1, 3, 4)\n","        x = self.spatial_decoder(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yMkv3igN_WAW"},"source":["X = np.random.rand(1,1,5,3)\n","X = np.round(X,3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Fr4Um0ekABnR","executionInfo":{"status":"ok","timestamp":1623262840134,"user_tz":-60,"elapsed":304,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"8a49c542-7089-4f1a-f478-f05604c42dbe"},"source":["p = os.getcwd()\n","path = os.path.join(p,'code/loly')\n","path"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/Thesis/code/loly'"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"VscUapEp_yK0"},"source":["with open(path,'wb') as f: pickle.dump(X, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KyjdcCK-Ac0X","executionInfo":{"status":"ok","timestamp":1623262892231,"user_tz":-60,"elapsed":268,"user":{"displayName":"marouaneBU Ferjani","photoUrl":"","userId":"08445040713790441888"}},"outputId":"d383e3eb-05db-424f-ffb1-0a9b1eda2bd0"},"source":["test = pd.read_pickle (path)\n","test.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 1, 5, 3)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"PUOITNe4eAqp"},"source":["# cell no 5\n","def one_epoch(net, loss, dl, opt=None, metric=None):\n","    \n","    if opt:\n","        net.train()  # only affects some layers\n","    else:\n","        net.eval()\n","        rq_stored = []\n","        for p in net.parameters():\n","            rq_stored.append(p.requires_grad)\n","            p.requires_grad = False\n","    \n","    L, M = [], []\n","    dl_it = iter(dl)\n","    for xb, yb in tqdm(dl_it, leave=False):\n","        xb, yb = xb.cuda(), yb.cuda()\n","        y_ = net(xb)\n","        l = loss(y_, yb)\n","        if opt:\n","            opt.zero_grad()\n","            l.backward()\n","            opt.step()\n","        L.append(l.detach().cpu().numpy())\n","        if metric: M.append(metric(y_, yb).cpu().numpy())\n","        \n","    if not opt:\n","        for p,rq in zip(net.parameters(), rq_stored): p.requires_grad = rq\n","            \n","    return L, M\n","\n","# cell no 7\n","def fit(net, tr_dl, val_dl, loss=nn.CrossEntropyLoss(), epochs=3, lr=3e-3, wd=1e-3, plot=True):   \n","    \n","    opt = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n","    \n","    Ltr_hist, Lval_hist = [], []\n","    for epoch in trange(epochs):\n","        Ltr,  _    = one_epoch(net, loss, tr_dl,  opt)\n","        Lval, Aval = one_epoch(net, loss, val_dl, None, accuracy)\n","        Ltr_hist.append(np.mean(Ltr))\n","        Lval_hist.append(np.mean(Lval))\n","        print(f'epoch: {epoch}\\ttraining loss: {np.mean(Ltr):0.4f}\\tvalidation loss: {np.mean(Lval):0.4f}\\tvalidation accuracy: {np.mean(Aval):0.2f}')\n","        \n","    # plot the losses     \n","    if plot:\n","        _,ax = plt.subplots(1,1,figsize=(16,4))\n","        ax.plot(1+np.arange(len(Ltr_hist)),Ltr_hist)\n","        ax.plot(1+np.arange(len(Lval_hist)),Lval_hist)\n","        ax.grid('on')\n","        ax.set_xlim(left=1, right=len(Ltr_hist))\n","        ax.legend(['training loss', 'validation loss']);\n","        \n","    return Ltr_hist, Lval_hist    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tWMKBXs0aDFs"},"source":["#accuracy = lambda y_,yb: (y_.max(dim=1)[1] == yb).float().mean()"],"execution_count":null,"outputs":[]}]}